<center>
    <img src="static/assets/AI-SDM-text.png" alt="AI-SDM Logo" style="max-width: 70%; height: auto; margin-bottom: 20px;">
</center>


<p>
    This is a reproduction of the Overcooked-AI environment (<a href="https://arxiv.org/abs/1910.05789">Carroll et al., 2019</a>) that uses <a href="https://cogrid.readthedocs.io/en/latest/">CoGrid</a> and <a href="https://interactive-gym.readthedocs.io/en/latest/">Interactive Gym</a>. 
</p>
<p>
    CoGrid and Interactive Gym were developed in the <a href="https://www.cmu.edu/dietrich/sds/ddmlab/">Dynamic Decision Making Lab</a> at Carnegie Mellon University. They comprise a framework for developing Human-AI interaction experiments, 
    where CoGrid is an easy-to-use library for building grid-based environments (in the <a href="https://gymnasium.farama.org/index.html">Gymnasium</a> or <a href="https://pettingzoo.farama.org/index.html">PettingZoo</a> formats) for multi-agent simulation and AI training. 
    Interactive Gym is a library for developing and running these experiments in a web browser, it provides the novel functionality to run these Python-based environments directly in the browser (alongside AI model inference using ONNX).
</p>
 
<p>  
    In this demo, you'll be playing a simplified version of the Overcooked video game. Your goal is to collaborate with your AI partner to cook and deliver as many dishes as possible before time runs out.
</p>

<p>
    Here's how the demo will work:
    <ol>
        <li>You'll complete a tutorial to learn the basics of the game.</li>
        <li>You'll play one 45-second round with an AI partner.</li>
        <li>Then, you'll play another 45-second round with a different AI partner.</li>
        <li>After these two rounds, you'll indicate which partner you preferred and provide an evaluation of the partners.</li>
    </ol>
    
    One of these AI partners is trained in standard self-play using deep reinforcement learning. This means that the AI partner has learned to play the game by playing against itself.
    
    
    <br><br>
    The other AI partner is trained using a new method we developed called Interpretable Behavior Conditioning (IBC). This method teaches the AI to adapt its behavior based on different sub-goals. During training, we randomly change these goals, which helps the AI learn to play well with many different types of partners. This makes the AI more flexible and able to work better with various human players. It also means we can adjust how the AI behaves by changing its sub-goals. We'll be sharing more details about this method in an upcoming paper.
</p>


<p>
    <b>Click "Continue" to start the tutorial.</b>
</p>

